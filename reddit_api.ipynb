{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pactools.grid_search import GridSearchCVProgressBar\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from psaw import PushshiftAPI\n",
    "#from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# classification models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(site_name='ga_proj3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosleep_gen = list(api.search_submissions(subreddit='nosleep',\n",
    "                                          limit=3000))\n",
    "\n",
    "nosleep_dict = {'id':[submission.id for submission in nosleep_gen],\n",
    "                'author':[submission.author for submission in nosleep_gen],\n",
    "               'title':[submission.title for submission in nosleep_gen],\n",
    "              'body':[submission.selftext for submission in nosleep_gen],\n",
    "               'subreddit':[submission.subreddit.display_name for submission in nosleep_gen]}\n",
    "\n",
    "nosleep_df_raw = pd.DataFrame(nosleep_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 5)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nosleep_df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           False\n",
       "author       False\n",
       "title        False\n",
       "body          True\n",
       "subreddit    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nosleep_df_raw.isin(['[removed]','[deleted]']).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosleep_df_clean = nosleep_df_raw[(nosleep_df_raw['body'] != '[removed]') &\n",
    "                                  (nosleep_df_raw['body'] != '[deleted]') &\n",
    "                                  (nosleep_df_raw['body'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1196, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nosleep_df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df = nosleep_df_clean.reset_index(drop=True)\n",
    "n_df.to_csv('nosleep.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "creepypasta_gen = list(api.search_submissions(subreddit='creepypasta',\n",
    "                                          limit=3000))\n",
    "\n",
    "creepypasta_dict = {'id':[submission.id for submission in creepypasta_gen],\n",
    "                'author':[submission.author for submission in creepypasta_gen],\n",
    "               'title':[submission.title for submission in creepypasta_gen],\n",
    "              'body':[submission.selftext for submission in creepypasta_gen],\n",
    "                   'subreddit':[submission.subreddit.display_name for submission in creepypasta_gen]}\n",
    "\n",
    "creepypasta_df_raw = pd.DataFrame(creepypasta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "creepypasta_df_clean = creepypasta_df_raw[(creepypasta_df_raw['body'] != '[removed]') &\n",
    "                                (creepypasta_df_raw['body'] != '[deleted]') &\n",
    "                                (creepypasta_df_raw['body'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creepypasta_df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = creepypasta_df_clean.reset_index(drop=True)\n",
    "c_df.to_csv('creepypasta.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c00vpg</td>\n",
       "      <td>OpinionatedIMO</td>\n",
       "      <td>Someone kept calling, and it was ‘me’</td>\n",
       "      <td>—————\\nThe phone rang. There was no caller ID ...</td>\n",
       "      <td>creepypasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c00cad</td>\n",
       "      <td>ChemoSans</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Catherine Miller is perfect. Her skin is flawl...</td>\n",
       "      <td>creepypasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bzzqfc</td>\n",
       "      <td>reddit_rats</td>\n",
       "      <td>Just remembered this</td>\n",
       "      <td>anyone know the creepypasta \"buyer beware\" i j...</td>\n",
       "      <td>creepypasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bzygc5</td>\n",
       "      <td>story_teller_64</td>\n",
       "      <td>Recess lost episode</td>\n",
       "      <td>So I went to a goodwill and found a recess DVD...</td>\n",
       "      <td>creepypasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bzwxdm</td>\n",
       "      <td>Chase-Covington</td>\n",
       "      <td>I Got A Phone Call From A Number That Doesn't ...</td>\n",
       "      <td>This started happening  2 days ago at 9:02 AM....</td>\n",
       "      <td>creepypasta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           author                                              title  \\\n",
       "0  c00vpg   OpinionatedIMO              Someone kept calling, and it was ‘me’   \n",
       "1  c00cad        ChemoSans                                            Perfect   \n",
       "2  bzzqfc      reddit_rats                               Just remembered this   \n",
       "3  bzygc5  story_teller_64                                Recess lost episode   \n",
       "4  bzwxdm  Chase-Covington  I Got A Phone Call From A Number That Doesn't ...   \n",
       "\n",
       "                                                body    subreddit  \n",
       "0  —————\\nThe phone rang. There was no caller ID ...  creepypasta  \n",
       "1  Catherine Miller is perfect. Her skin is flawl...  creepypasta  \n",
       "2  anyone know the creepypasta \"buyer beware\" i j...  creepypasta  \n",
       "3  So I went to a goodwill and found a recess DVD...  creepypasta  \n",
       "4  This started happening  2 days ago at 9:02 AM....  creepypasta  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1423, 5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1196, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.concat([n_df,c_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df['text'] = agg_df['title'] +  ' ' + agg_df['body']\n",
    "agg_df['creepypasta'] = agg_df['subreddit'].map(lambda x: 1 if  x == 'creepypasta' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.543337\n",
       "0    0.456663\n",
       "Name: creepypasta, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True) # we observe some imbalance here, with slightly more representation of creepypasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.extend(['http','com','net','org','www','https'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regcleaner(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    meaningful_words = [w for w in words if not w in stop]\n",
    "    \n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "agg_df['cleantext'] = agg_df['text'].map(lambda x:regcleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>creepypasta</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c020my</td>\n",
       "      <td>OneFaraday</td>\n",
       "      <td>The Book of Autonomancy (Part 10)</td>\n",
       "      <td>[Part 1](https://www.reddit.com/r/nosleep/comm...</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>The Book of Autonomancy (Part 10) [Part 1](htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>book autonomancy part 10 part 1 reddit r nosle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c01yge</td>\n",
       "      <td>CrazybloxianEmpireNS</td>\n",
       "      <td>I think my brother found images that kill you ...</td>\n",
       "      <td>[PART 1](https://www.reddit.com/r/nosleep/comm...</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>I think my brother found images that kill you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>think brother found images kill part 2 part 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c01y1o</td>\n",
       "      <td>thayeryan</td>\n",
       "      <td>Every night at 11:11, they call.</td>\n",
       "      <td>\\r\\n\\r\\nEvery night at 11:11, they call. I do...</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>Every night at 11:11, they call.  \\r\\n\\r\\nEver...</td>\n",
       "      <td>0</td>\n",
       "      <td>every night 11 11 call every night 11 11 call ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c01vnc</td>\n",
       "      <td>edelre28</td>\n",
       "      <td>The True Disney Working Experience</td>\n",
       "      <td>My name is Neil. I’ve been a walk around chara...</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>The True Disney Working Experience My name is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>true disney working experience name neil walk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c01px3</td>\n",
       "      <td>awsmithwrites</td>\n",
       "      <td>Don't Fall Asleep in a Russian Subway Station.</td>\n",
       "      <td>“Are you reading *Dead Souls?*” the blonde gir...</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>Don't Fall Asleep in a Russian Subway Station....</td>\n",
       "      <td>0</td>\n",
       "      <td>fall asleep russian subway station reading dea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                author  \\\n",
       "0  c020my            OneFaraday   \n",
       "1  c01yge  CrazybloxianEmpireNS   \n",
       "2  c01y1o             thayeryan   \n",
       "3  c01vnc              edelre28   \n",
       "4  c01px3         awsmithwrites   \n",
       "\n",
       "                                               title  \\\n",
       "0                  The Book of Autonomancy (Part 10)   \n",
       "1  I think my brother found images that kill you ...   \n",
       "2                   Every night at 11:11, they call.   \n",
       "3                 The True Disney Working Experience   \n",
       "4     Don't Fall Asleep in a Russian Subway Station.   \n",
       "\n",
       "                                                body subreddit  \\\n",
       "0  [Part 1](https://www.reddit.com/r/nosleep/comm...   nosleep   \n",
       "1  [PART 1](https://www.reddit.com/r/nosleep/comm...   nosleep   \n",
       "2   \\r\\n\\r\\nEvery night at 11:11, they call. I do...   nosleep   \n",
       "3  My name is Neil. I’ve been a walk around chara...   nosleep   \n",
       "4  “Are you reading *Dead Souls?*” the blonde gir...   nosleep   \n",
       "\n",
       "                                                text  creepypasta  \\\n",
       "0  The Book of Autonomancy (Part 10) [Part 1](htt...            0   \n",
       "1  I think my brother found images that kill you ...            0   \n",
       "2  Every night at 11:11, they call.  \\r\\n\\r\\nEver...            0   \n",
       "3  The True Disney Working Experience My name is ...            0   \n",
       "4  Don't Fall Asleep in a Russian Subway Station....            0   \n",
       "\n",
       "                                           cleantext  \n",
       "0  book autonomancy part 10 part 1 reddit r nosle...  \n",
       "1  think brother found images kill part 2 part 1 ...  \n",
       "2  every night 11 11 call every night 11 11 call ...  \n",
       "3  true disney working experience name neil walk ...  \n",
       "4  fall asleep russian subway station reading dea...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = agg_df['cleantext'], agg_df['creepypasta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                  random_state = 42,\n",
    "                                                  stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.8s finished\n",
      "C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:71: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n",
      "  return matrix(data, dtype=dtype, copy=False)\n",
      "C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\imblearn\\utils\\deprecation.py:53: DeprecationWarning: 'ratio' is deprecated from 0.4 and will be removed in 0.6 for the estimator <class 'imblearn.over_sampling._smote.SMOTE'>. Use 'sampling_strategy' instead.\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "cvec = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                      stop_words='english')\n",
    "smt = SMOTE(random_state=42,ratio=1)\n",
    "ss = StandardScaler()\n",
    "mnb = MultinomialNB()\n",
    "pipeline = Pipeline([('cvec',cvec),\n",
    "                     ('smt',smt),\n",
    "                    ('mnb',mnb)])\n",
    "score = cross_val_score(pipeline, X_train, y_train, cv=5, verbose=1, n_jobs=-1)\n",
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.87      0.71       299\n",
      "           1       0.83      0.52      0.64       356\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       655\n",
      "   macro avg       0.71      0.69      0.67       655\n",
      "weighted avg       0.72      0.68      0.67       655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68274112, 0.70558376, 0.68112245, 0.71428571, 0.64285714])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853180358437791"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6778625954198473"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    9.3s finished\n",
      "C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:71: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n",
      "  return matrix(data, dtype=dtype, copy=False)\n",
      "C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\imblearn\\utils\\deprecation.py:53: DeprecationWarning: 'ratio' is deprecated from 0.4 and will be removed in 0.6 for the estimator <class 'imblearn.over_sampling._smote.SMOTE'>. Use 'sampling_strategy' instead.\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "tfv = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                      stop_words='english')\n",
    "smt = SMOTE(random_state=42,ratio=1)\n",
    "ss = StandardScaler()\n",
    "mnb = MultinomialNB()\n",
    "pipeline = Pipeline([('tfv',tfv),\n",
    "                     ('smt',smt),\n",
    "                    ('mnb',mnb)])\n",
    "score = cross_val_score(pipeline, X_train, y_train, cv=5, verbose=1, n_jobs=-1)\n",
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.69       299\n",
      "           1       1.00      0.24      0.38       356\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       655\n",
      "   macro avg       0.76      0.62      0.53       655\n",
      "weighted avg       0.78      0.58      0.52       655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58375635, 0.58883249, 0.58928571, 0.60969388, 0.54591837])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5834973583341967"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5847328244274809"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_summary(X_train, X_test, y_train, y_test, model_name, model, model_params, problem = 'classification'):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X_train, X_test, y_train, y_test : vectorized train test split of X and y\n",
    "    model_name : str name of model\n",
    "    model : model constructor\n",
    "        example: 'LogisticRegression' : LogisticRegression()\n",
    "    model_params : dictionary of param_grids for GridSearch\n",
    "        example: 'LogisticRegression' : {\n",
    "                      'penalty' : ['l1', 'l2'],\n",
    "                      'C' : [.1, 1, 10] }\n",
    "    problem : str of problem type: 'classification' or 'regression'\n",
    "    \n",
    "    Return:\n",
    "    summary_df : a single row DataFrame containing the GridSearch model and its \n",
    "              best model, predictions, and scores.\n",
    "    '''   \n",
    "    problem = problem.lower()\n",
    "    \n",
    "    if problem != 'regression' and problem != 'classification':\n",
    "        print('Invalid problem type. Try \"regression\" or \"classification\"')\n",
    "        return\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    # Track progress\n",
    "    print(f'Fitting {model_name}')\n",
    "    \n",
    "    # Pipeline\n",
    "    model = model\n",
    "    \n",
    "    tfv = TfidfVectorizer(tokenizer=RegexpTokenizer,\n",
    "                        stop_words=stop,\n",
    "                        sublinear_tf=True)\n",
    "    \n",
    "    pipeline = Pipeline([('tfv', tfv),\n",
    "                       ('scaler', StandardScaler()),\n",
    "                       ('SMOTE', SMOTE()),\n",
    "                       ('model', model)])\n",
    "    for key in model_params.keys():\n",
    "        model_params['model__'+key] = model_params.pop(key)\n",
    "    model_params['tfv__ngram_range'] = [zip([1,1,1,1,2,2,2,3,3,4],[1,2,3,4,2,3,4,3,4,4])]\n",
    "    model_params['tfv__max_df'] = np.arange(0.5,1.1,0.1)\n",
    "\n",
    "    # GridSearch\n",
    "    gs = GridSearchCVProgressBar(pipeline, model_params, cv = 5, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = gs.best_estimator_.predict(X_train)\n",
    "    y_test_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Build summary\n",
    "    summary['Model Name'] = model_name\n",
    "    summary['Train Pred'] = y_train_pred\n",
    "    summary['Test Pred'] = y_test_pred\n",
    "    summary['Best Score'] = gs.best_score_\n",
    "    summary['Best Params'] = gs.best_params_\n",
    "    summary['Best Estimator'] = gs.best_estimator_\n",
    "    summary['Grid Search Model'] = gs\n",
    "\n",
    "    if problem == 'regression':\n",
    "        summary['Train Score'] = r2_score(y_train, y_train_pred)\n",
    "        summary['Test Score'] = r2_score(y_test, y_test_pred)\n",
    "    elif problem == 'classification':\n",
    "        summary['Train Score'] = accuracy_score(y_train, y_train_pred)\n",
    "        summary['Test Score'] = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Construct output dataframe \n",
    "    summary_df = pd.DataFrame([summary])\n",
    "\n",
    "    # Rearrange columns\n",
    "    summary_df = summary_df[['Model Name', 'Best Params', 'Best Score', 'Best Estimator',\n",
    "                             'Train Score', 'Test Score', 'Train Pred', 'Test Pred', 'Grid Search Model']]\n",
    "        \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_models = {\n",
    "    'LogisticRegression' : LogisticRegression(random_state = 42),\n",
    "    'KNN': KNeighborsClassifier(), \n",
    "    'NaiveBayes' : MultinomialNB(),\n",
    "    'DecisionTree' : DecisionTreeClassifier(random_state = 42), \n",
    "    'BaggedDecisionTree' : BaggingClassifier(random_state = 42),\n",
    "    'RandomForest' : RandomForestClassifier(random_state = 42), \n",
    "    'ExtraTrees' : ExtraTreesClassifier(random_state = 42), \n",
    "    'AdaBoost' : AdaBoostClassifier(random_state=42), \n",
    "    'GradientBoosting' : GradientBoostingClassifier(random_state = 42),\n",
    "    'SVM' : SVC(random_state=42),\n",
    "    'XGBoost' : XGBClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model_params = {\n",
    "    'LogisticRegression' : {\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : np.arange(.05, 1, .05) },\n",
    "    'KNN' : {\n",
    "        'n_neighbors' : np.arange(3, 22, 2) },\n",
    "    'NaiveBayes' : {\n",
    "        'alpha' : np.arange(.05, 2, .05)},\n",
    "    'DecisionTree': {\n",
    "        'max_depth' : [None, 6, 10, 14], \n",
    "        'min_samples_leaf' : [1, 2],\n",
    "        'min_samples_split': [2, 3] },\n",
    "    'BaggedDecisionTree' : {\n",
    "        'n_estimators' : [20, 60, 100] },\n",
    "    'RandomForest' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 2, 6, 10],\n",
    "        'min_samples_split' : [2, 3, 4] },\n",
    "    'ExtraTrees' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 6, 10, 14],\n",
    "        'min_samples_leaf' : [1, 2], \n",
    "        'min_samples_split' : [2, 3], },\n",
    "    'AdaBoost' : {\n",
    "        'n_estimators' : np.arange(100, 151, 25),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 20) },\n",
    "    'GradientBoosting' : {\n",
    "        'n_estimators' : np.arange(5, 150, 10),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 20),\n",
    "        'max_depth' : [1, 2, 3] },\n",
    "    'SVM' : {\n",
    "        'C' : np.arange(0.05, 1, .05),\n",
    "        'kernel' : ['rbf', 'linear'] },\n",
    "    'XGBoost' : {\n",
    "        'n_estimators'  : np.arange(100, 151, 25), \n",
    "        'learning_rate' : np.arange(0.1, 1, .3),\n",
    "        'max_depth' : [3],\n",
    "        'alpha' : np.arange(0, 1, .3),\n",
    "        'lambda' : np.arange(0, 1, .3),\n",
    "        'gamma' : np.arange(0, 1, .3),\n",
    "        'subsample' : [.5],\n",
    "        'n_jobs' : [4],\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_summaries = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LogisticRegression\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter model for estimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 514, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 147, in set_params\n    self._set_params('steps', **kwargs)\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 52, in _set_params\n    super(_BaseComposition, self).set_params(**params)\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 222, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"C:\\Users\\Jay\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 213, in set_params\n    (key, self))\nValueError: Invalid parameter model for estimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-1dd86e75eae1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                             \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                             classifier_model_params[model]), \n\u001b[0m\u001b[0;32m     11\u001b[0m                             ignore_index = True)\n",
      "\u001b[1;32m<ipython-input-146-3706acb52ae4>\u001b[0m in \u001b[0;36mgridsearch_summary\u001b[1;34m(X_train, X_test, y_train, y_test, model_name, model, model_params, problem)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# GridSearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCVProgressBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter model for estimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "models = ['LogisticRegression', 'KNN']\n",
    "\n",
    "for model in models:\n",
    "    gs_summaries = gs_summaries.append(\n",
    "                        gridsearch_summary(\n",
    "                            X_train, X_test, y_train, y_test, \n",
    "                            model, classifier_models[model], \n",
    "                            classifier_model_params[model]), \n",
    "                            ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LogisticRegression', 'KNN', 'NaiveBayes', \n",
    "          'DecisionTree', 'BaggedDecisionTree', \n",
    "          'RandomForest', 'ExtraTrees', 'AdaBoost', 'SVM',\n",
    "          'GradientBoosting', 'XGBoost']\n",
    "\n",
    "for model in models:\n",
    "    gs_summaries = gs_summaries.append(\n",
    "                        gridsearch_summary(\n",
    "                            X_train, X_test, y_train, y_test, \n",
    "                            model, classifier_models[model], \n",
    "                            classifier_model_params[model]), \n",
    "                            ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Remove stop words.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and parsing the training set movie reviews...\")\n",
    "\n",
    "j = 0\n",
    "for train_review in X_train['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_train_reviews.append(review_to_words(train_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "\n",
    "print(\"Cleaning and parsing the testing set movie reviews...\")\n",
    "\n",
    "for test_review in X_test['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_test_reviews.append(review_to_words(test_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "        \n",
    "    j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
